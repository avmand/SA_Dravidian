
"""

Citation:
@inproceedings{ bitsnlp2021dravidian,
 title={Sentiment Analysis of Dravidian Code Mixed Data},
 author={Mandalam, Asrita Venkata and Sharma, Yashvardhan},
 booktitle={ 15th Conference of the European Chapter of the Association for Computational Linguistics 2021 (EACL 2021) },
 year={2021}
}  

"""

import numpy as np
import pandas as pd
import re
from keras.layers import LSTM, Dense, Embedding, Input
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import keras

from tqdm import tqdm
tqdm.pandas(desc="progress-bar")
from gensim.models.word2vec import Word2Vec
from gensim.models.doc2vec import TaggedDocument
import multiprocessing
from sklearn import utils
from keras.models import Model
from keras.layers import BatchNormalization, Activation


def token(sentence):
    tokens = []
    for t in re.findall("[a-zA-Z]+",sentence):
         if len(t)>=1:
             tokens.append(t)
    return tokens

def emoji_ch (words):
    if words == 'üòÑ' :
      words='good'
    elif words == 'üòÜ' :
      words='good'
    elif words == 'üòä' :
      words='good'
    elif words == 'üòÉ' :
      words='good'	
    elif words == 'ü§¨' :
      words='bad'
    elif words == 'üòè' :
      words='good'	
    elif words == 'üòç' :
      words='good'	
    elif words == 'üòò' :
      words='good'
    elif words == 'üòö' :
      words='good'	
    elif words == 'üò≥' :
      words='flushed'	
    elif words == 'üòå' :
      words='bad'
    elif words == 'üòÜ' :
      words='good'	
    elif words == 'üòÇ' :
      words='good'	
    elif words == 'üòé' :
      words='good'		
    elif words == 'ü§Ø' :
      words='blow_mind'	
    elif words == '‚úåÔ∏è' :
      words='vhand'			
    elif words == 'ü§ò' :
      words='metal'
    elif words == 'ü§¶' :
      words='bad'				
    elif words == 'ü§©':
      words='good'
    elif words == 'ü•∞':
      words='good'
    elif words == 'ü§î':
      words='thinking'
    elif words == 'ü§£':
      words='good'
    elif words == 'ü§ó':
      words = 'good'
    else :
      return None
    return words+" "


def tamilchar(ch):
    if ch == u'\u0B85' or ch== u'‡ÆÖ':
        ch="A"
    elif ch == u'\u0B86' or ch== u'‡ÆÜ':
        ch="AA"
    elif ch == u'\u0B87' or ch== u'‡Æá':
        ch="I"
    elif ch == u'\u0B88' or ch== u'‡Æà':
        ch="II"
    elif ch == u'\u0B89' or ch== u'‡Æâ':
        ch="U"
    elif ch == u'\u0B8A' or ch== u'‡Æä':
        ch="UU"
    elif ch == u'\u0B8E' or ch== u'‡Æé':
        ch="E"
    elif ch == u'\u0B8F' or ch== u'‡Æè':
        ch="EE"
    elif ch == u'\u0B90' or ch== u'‡Æê':
        ch="AI"
    elif ch == u'\u0B92' or ch== u'‡Æí':
        ch="O"
    elif ch == u'\u0B93' or ch== u'‡Æì':
        ch="OO"
    elif ch == u'\u0B94' or ch== u'‡Æî':
        ch="AU"
    elif ch ==u'\u0B95'or ch==u'‡Æï':
        ch="KA"
    elif ch ==u'\u0B99'or ch==u'‡Æô':
        ch="NGA"
    elif ch ==u'\u0B9A'or ch==u'‡Æö':
        ch="SA"
    elif ch ==u'\u0B9C' or ch==u'‡Æú':
        ch="JA"
    elif ch ==u'\u0B9E'or ch==u'‡Æû':
        ch=u"NYA"
    elif ch ==u'\u0B9F'or ch==u'‡Æü':
        ch="DA"
    elif ch ==u'\u0BA3'or ch==u'‡Æ£':
        ch="NNA"
    elif ch ==u'\u0BA4'or ch==u'‡Æ§':
        ch="THA"
    elif ch ==u'\u0BA8'or ch==u'‡Æ®':
        ch="NA"
    elif ch ==u'\u0BA9' or ch==u'‡Æ©':
        ch="NA"
    elif ch ==u'\u0BAA'or ch==u'‡Æ™':
        ch="PA"
    elif ch ==u'\u0BAE'or ch==u'‡ÆÆ':
        ch="MA"
    elif ch ==u'\u0BAF' or ch==u'‡ÆØ':
        ch="YA"
    elif ch ==u'\u0BB0' or ch==u'‡Æ∞':
        ch="RA"
    elif ch ==u'\u0BB1'or ch==u'‡Æ±':
        ch="RRA"
    elif ch ==u'\u0BB2' or ch==u'‡Æ≤':
        ch="LA"
    elif ch ==u'\u0BB3'or ch==u'‡Æ≥':
        ch="LLA"
    elif ch ==u'\u0BB4'or ch ==u'‡Æ¥':
        ch=="LLLA"
    elif ch ==u'\u0BB5'or ch==u'‡Æµ':
        ch="VA"
    elif ch ==u'\u0BB6'or ch==u'‡Æ∂':
        ch="SHA"
    elif ch ==u'\u0BB7'or ch==u'‡Æ∑':
        ch="SSA"
    elif ch ==u'\u0BB8'or ch==u'‡Æ∏':
        ch="SA"
    elif ch ==u'\u0BB9'or ch==u'‡Æπ':
        ch="HA"
    elif ch ==u'\u0BBE'or ch==u'$‡Ææ':
        ch="AA"
    elif ch ==u'\u0BBF'or ch==u'$‡Æø':
        ch="I"
    elif ch ==u'\u0BC0'or ch==u'$‡ØÄ':
        ch="II"
    elif ch ==u'\u0BC1'or ch==u'$‡ØÅ':
        ch="U"
    elif ch ==u'\u0BC2'or ch==u'‡ØÇ$':
        ch="UU"
    elif ch ==u'\u0BC6'or ch==u'$‡ØÜ':
        ch="E"
    elif ch ==u'\u0BC7'or ch==u'$‡Øá':
        ch="EE"
    elif ch ==u'\u0BC8'or ch==u'$‡Øà':
        ch="AI"
    elif ch ==u'\u0BCA'or ch==u'$‡Øä':
        ch="O"
    elif ch ==u'\u0BCB'or ch==u'$‡Øã':
        ch="OO"
    elif ch ==u'\u0BCC'or ch==u'$‡Øå':
        ch="AU"
    elif ch ==u'\u0BCD'or ch==u'$‡Øç':
        ch="PULLI"
    elif ch ==u'\u0BD0'or ch==u'‡Øê':
        ch="OM"
    elif ch ==u'\u0BD7'or ch==u'$‡Øó':
        ch="AU"       
    else:
        return None
    return ch

def preprocess(word):
  word=re.sub(r'([a-z])\1+', r'\1',word)
  return word

def tamilword(word):
    fla=0
    string=""
    for ch in word:
        if(tamilchar(ch) is not None):
            ch=tamilchar(ch)
            fla=1
        if(emoji_ch(ch) is not None):
            ch=emoji_ch(ch)
            fla=1
        string=string+str(ch)
    if(fla==1):
        return string
    else:
        return None


def malchar(ch):
    if ch == u'‡¥Ö':
        ch="A"
    elif ch == u'‡¥Ü':
        ch="AA"
    elif ch == u'‡¥á':
        ch="I"
    elif ch == u'‡¥à':
        ch="II"
    elif ch == u'‡¥â':
        ch="U"
    elif ch == u'‡¥ä':
        ch="UU"
    elif ch == u'‡¥é':
        ch="E"
    elif ch == u'‡¥è':
        ch="EE"
    elif ch == u'‡¥ê':
        ch="AI"
    elif ch == u'‡¥í':
        ch="O"
    elif ch == u'‡¥ì':
        ch="OO"
    elif ch == u'‡¥î':
        ch="AU"
    elif ch ==u'‡¥ï'or ch==u'‡¥ñ':
        ch="KA"
    elif ch ==u'‡¥ó'or ch==u'‡¥ò':
        ch="GA"
    elif ch ==u'‡¥ô':
        ch="NGA"
    elif ch ==u'‡¥ö'or ch==u'‡¥õ':
        ch="CH" 
    elif ch ==u'‡¥ö'or ch==u'‡¥õ':
        ch="CH" 
    elif ch ==u'‡¥∂'or ch==u'‡¥∑':
        ch="SH"
    elif ch ==u'‡¥∏':
        ch="S"
    elif ch ==u'‡¥ú' or ch==u'‡¥ù':
        ch="JA"
    elif ch ==u'‡¥°'or ch==u'‡¥¶':
        ch="DA"
    elif ch ==u'‡¥•'or ch==u'‡¥§':
        ch="THA"
    elif ch ==u'‡¥£'or ch==u'‡¥®':
        ch="NA"
    elif ch ==u'‡¥™':
        ch="PA"
    elif ch ==u'‡¥Æ':
        ch="MA"
    elif ch ==u'‡¥Ø':
        ch="YA"
    elif ch ==u'‡¥∞':
        ch="RA"
    elif ch ==u'‡¥≥':
        ch="LA"
    elif ch ==u'‡¥µ':
        ch="VA"
    elif ch ==u'‡¥π':
        ch="HA"
    elif ch ==u'‡¥≠'or ch==u'‡¥¨':
        ch="BA"
    elif ch ==u'‡¥¢'or ch==u'‡¥ß':
        ch="DH"
    elif ch ==u'‡¥†':
        ch="DT"
    elif ch ==u'‡¥ü':
        ch="T"
    elif ch ==u'‡¥û':
        ch="NJ"
    elif ch ==u'‡¥´':
        ch="PH"
    elif ch ==u'‡¥¥':
        ch="ZH"
    elif ch ==u'‡¥±':
        ch="RA"    
    else:
        return None
    return ch


def malword(word):
    fla=0
    string=""
    for ch in word:
        if(malchar(ch) is not None):
            ch=malchar(ch)
            fla=1
        string=string+str(ch)
    if(fla==1):
        return string
    else:
        return None



colnames=['text','label']
df = pd.read_csv(r'/content/drive/My Drive/Dravidian/EACL/tamil_train.tsv',names=colnames, delimiter='\t', error_bad_lines=False, header=None,
                      usecols=['text','label'], na_values=" NaN",skiprows=[0])

df_val = pd.read_csv(r'/content/drive/My Drive/Dravidian/EACL/tamil_dev.tsv',names=colnames, delimiter='\t', error_bad_lines=False, header=None,
                      usecols=['text','label'], na_values=" NaN",skiprows=[0])

colnames=['index','text','label']
df_test = pd.read_csv(r'/content/drive/My Drive/Dravidian/EACL/tamil_test.tsv',names=colnames, delimiter='\t', error_bad_lines=False, header=None,
                      usecols=['text','label'], na_values=" NaN",skiprows=[0])

# colnames=['text','label']
# df = pd.read_csv(r'/content/drive/MyDrive/Dravidian/EACL/malayalam_train.tsv',names=colnames, delimiter='\t', error_bad_lines=False, header=None,
#                       usecols=['text','label'], na_values=" NaN",skiprows=[0])

# df_val = pd.read_csv(r'/content/drive/MyDrive/Dravidian/EACL/malayalam_dev.tsv',names=colnames, delimiter='\t', error_bad_lines=False, header=None,
#                       usecols=['text','label'], na_values=" NaN",skiprows=[0])

# colnames=['index','text','label']
# df_test = pd.read_csv(r'/content/drive/MyDrive/Dravidian/EACL/malayalam_test.tsv',names=colnames, delimiter='\t', error_bad_lines=False, header=None,
#                       usecols=['text','label'], na_values=" NaN",skiprows=[0])

labels = ['Mixed_feelings ','Negative ','Positive ','not-Tamil ','unknown_state '] 

for index, line in df.iterrows():
  if line['label'].lower() == labels[0].lower():
        line['label'] = 0
  elif line['label'].lower() == labels[1].lower():
        line['label'] = 1
  elif line['label'].lower() == labels[2].lower():
        line['label'] = 2
  elif line['label'].lower() == labels[3].lower():
        line['label'] = 3
  elif line['label'].lower() == labels[4].lower():
        line['label'] = 4
  i=line[0].split()
  for word in range(len(i)):
      for ch in range(len(i[word])):
          checker=emoji_ch(i[word][ch])
          if(checker is not None):
              i[word]+=checker
      
      checker=tamilword(i[word])
      if(checker is not None):
          i[word]=checker
      i[word]=preprocess(i[word])
  line[0]=' '.join(i)

for index, line in df_val.iterrows():
  if line['label'].lower() == labels[0].lower():
        line['label'] = 0
  elif line['label'].lower() == labels[1].lower():
        line['label'] = 1
  elif line['label'].lower() == labels[2].lower():
        line['label'] = 2
  elif line['label'].lower() == labels[3].lower():
        line['label'] = 3
  elif line['label'].lower() == labels[4].lower():
        line['label'] = 4
  i=line[0].split()
  for word in range(len(i)):
      for ch in range(len(i[word])):
          checker=emoji_ch(i[word][ch])
          if(checker is not None):
              i[word]+=checker
      
      checker=tamilword(i[word])
      if(checker is not None):
          i[word]=checker
      i[word]=preprocess(i[word])
  line[0]=' '.join(i)

labels = ['Mixed_feelings','Negative','Positive','not-Tamil','unknown_state'] 
for index, line in df_test.iterrows():
  if line['label'].lower() == labels[0].lower():
        line['label'] = 0
  elif line['label'].lower() == labels[1].lower():
        line['label'] = 1
  elif line['label'].lower() == labels[2].lower():
        line['label'] = 2
  elif line['label'].lower() == labels[3].lower():
        line['label'] = 3
  elif line['label'].lower() == labels[4].lower():
        line['label'] = 4
  i=line[0].split()
  for word in range(len(i)):
      for ch in range(len(i[word])):
          checker=emoji_ch(i[word][ch])
          if(checker is not None):
              i[word]+=checker
      i[word]=preprocess(i[word])
      
      checker=tamilword(i[word])
      if(checker is not None):
          i[word]=checker
      i[word]=preprocess(i[word])
          
          
  line[0]=' '.join(i)

print(df.head())
print(df_val.head())
print(df_test.head())

x_train = df['text']
y_train = df['label']
x_dev = df_val['text']
y_dev = df_val['label']
x_test = df_test['text']
y_test = df_test['label']

from collections import Counter
unique = Counter()
df['text'].str.lower().str.split().apply(unique.update)
avl=[]
for a in df['text']:
  avl.append(len(a))


import math
cbowsize=100
sgsize=100
totalsize=cbowsize+sgsize
maxlen=math.ceil(sum(avl)/len(avl))
tokmaxwords=len(unique)
batch_size=128


def LabelComments(tweets,label):
    result = []
    prefix = label
    for i, t in zip(tweets.index, tweets):
        result.append(TaggedDocument(t.split(), [prefix + '_%s' % i]))
    return result

trainTotal = x_train
trainTotalAll = LabelComments(trainTotal, 'all')

cores = multiprocessing.cpu_count()
CBOW_Model = Word2Vec(sg=0, size=cbowsize, negative=10, window=15, min_count=2, workers=cores, alpha=0.1, min_alpha=0.001)
CBOW_Model.build_vocab([x.words for x in tqdm(trainTotalAll)])
for epoch in range(10):
    CBOW_Model.train(utils.shuffle([x.words for x in tqdm(trainTotalAll)]), total_examples=len(trainTotalAll), epochs=1)

from gensim.models import FastText
FASTTEXT_Model = FastText([x.words for x in tqdm(trainTotalAll)], size=sgsize, window=15, workers=4,sg=1,negative=10,iter=10)

embeddings_index = {}
for w in CBOW_Model.wv.vocab.keys():
  if(w in FASTTEXT_Model.wv.vocab.keys()):
    embeddings_index[w] = np.append(CBOW_Model.wv[w],FASTTEXT_Model.wv[w])


tokenizer = Tokenizer(num_words=tokmaxwords)
tokenizer.fit_on_texts(x_train)
sequences = tokenizer.texts_to_sequences(x_train)
x_train_sequences = pad_sequences(sequences, maxlen=maxlen)

sequences_val = tokenizer.texts_to_sequences(x_dev)
x_val_sequences= pad_sequences(sequences_val, maxlen=maxlen)

sequences_test = tokenizer.texts_to_sequences(x_test)
x_test_sequences = pad_sequences(sequences_test, maxlen=maxlen)

num_words = tokmaxwords
embedding_matrix = np.zeros((num_words, totalsize))
for word, i in tokenizer.word_index.items():
    if i >= num_words:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

def LSTM_model(max_sequence_length, max_words_fn, embedding_dim, numberOfClasses):
    embedding_layer = Embedding(max_words_fn,embedding_dim,input_length=max_sequence_length,trainable=False,weights=[embedding_matrix])
    sequence_input = Input(shape=(max_sequence_length,), dtype='float32')
    embedded_sequences = embedding_layer(sequence_input)
    layer1 = LSTM(64, return_sequences=True, dropout = 0.3, recurrent_dropout=0.2)(embedded_sequences)
    layer2 = LSTM(32, dropout = 0.3, recurrent_dropout =0.2)(layer1)
    layer3 = Dense(numberOfClasses)(layer2)
    layer4 = BatchNormalization()(layer3)
    preds = Activation('softmax')(layer4)
    model = Model(sequence_input, preds)
    model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])
    model.summary()
    return model

max_words = tokmaxwords
maxlen = maxlen
embedding_dims = totalsize
model = LSTM_model(maxlen, max_words, embedding_dims, 5)
num_epochs = 500

y_train2 = np.asarray(y_train).astype('float32').reshape((-1,1))
y_val2 = np.asarray(y_dev).astype('float32').reshape((-1,1))

es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)
history = model.fit(x_train_sequences, y_train2, batch_size=batch_size,
                  callbacks=[es_callback], epochs=num_epochs, validation_data=(x_val_sequences, y_val2), shuffle=True,verbose=1)

from keras.utils import np_utils
from sklearn.metrics import classification_report
y_test3 = np_utils.to_categorical(y_test, 5)
predictions = model.predict(x_test_sequences)
prediction = (predictions>0.20) 
print(classification_report(y_test3, prediction))
